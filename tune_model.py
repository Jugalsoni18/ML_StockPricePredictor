import pandas as pd
import numpy as np
import pandas_ta as ta
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from xgboost import XGBRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
from typing import Any, Tuple

# Import the existing DataFetcher to get stock data
from datafetcher import DataFetcher

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

def create_features(data: pd.DataFrame) -> pd.DataFrame:
    """
    Create a rich set of features from the stock data using pandas-ta.
    """
    df = data.copy()
    
    # Add a comprehensive set of technical indicators
    df.ta.sma(length=20, append=True)
    df.ta.ema(length=20, append=True)
    df.ta.rsi(length=14, append=True)
    df.ta.macd(append=True)
    df.ta.bbands(append=True)
    df.ta.atr(append=True)
    df.ta.adx(append=True)
    
    # Add custom features
    df['Daily_Return'] = df['Close'].pct_change()
    df['Volatility'] = df['Daily_Return'].rolling(window=20).std()
    
    # Add lag features for the closing price
    for i in range(1, 6):
        df[f'Close_Lag_{i}'] = df['Close'].shift(i)
    
    # The target variable is the next day's closing price
    df['Target'] = df['Close'].shift(-1)
    
    # Remove rows with NaN values that were generated by the feature creation
    df = df.dropna()
    
    return df

def plot_feature_importance(model, features: list, model_name: str):
    """
    Generate and display a feature importance plot for the trained model.
    """
    importances = pd.Series(model.feature_importances_, index=features)
    top_20_importances = importances.nlargest(20).sort_values(ascending=True)
    
    plt.figure(figsize=(12, 8))
    sns.barplot(x=top_20_importances, y=top_20_importances.index, palette='viridis')
    plt.title(f'Top 20 Feature Importances for {model_name}', fontsize=16)
    plt.xlabel('Importance Score', fontsize=12)
    plt.ylabel('Features', fontsize=12)
    plt.tight_layout()
    plt.show()

def run_tuning(X_train_scaled: np.ndarray, y_train: pd.Series, model_type: str) -> Tuple[Any, dict]:
    """
    Run hyperparameter tuning for the specified model type.
    """
    # Use TimeSeriesSplit for cross-validation to respect the temporal order of data
    tscv = TimeSeriesSplit(n_splits=5)

    if model_type == 'RandomForest':
        model = RandomForestRegressor(random_state=42)
        param_grid = {
            'n_estimators': [100, 200],
            'max_depth': [10, 20, None],
            'min_samples_split': [2, 5],
            'min_samples_leaf': [1, 2]
        }
    elif model_type == 'XGBoost':
        model = XGBRegressor(random_state=42, objective='reg:squarederror')
        param_grid = {
            'n_estimators': [100, 300],
            'max_depth': [3, 5, 7],
            'learning_rate': [0.01, 0.1],
            'subsample': [0.8, 1.0]
        }
    else:
        raise ValueError("Unsupported model type. Choose 'RandomForest' or 'XGBoost'.")

    print(f"Running GridSearchCV for {model_type}...")
    grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=tscv, n_jobs=-1, scoring='r2', verbose=1)
    grid_search.fit(X_train_scaled, y_train)
    
    return grid_search.best_estimator_, grid_search.best_params_

def main(ticker: str = "AAPL"):
    """
    Main function to run the complete tuning and evaluation pipeline.
    """
    print(f"Starting model tuning pipeline for ticker: {ticker}")
    
    # 1. Fetch Data
    print("Step 1: Fetching data...")
    try:
        data_fetcher = DataFetcher()
        stock_data, _ = data_fetcher.fetch_stock_data(ticker)
        print(f"Successfully fetched {len(stock_data)} data points.")
    except Exception as e:
        print(f"Error fetching data: {e}")
        return

    # 2. Feature Engineering
    print("\nStep 2: Creating features...")
    df_features = create_features(stock_data)
    
    # 3. Define Features (X) and Target (y), then split
    features = [col for col in df_features.columns if col not in ['Target', 'Date']]
    X = df_features[features]
    y = df_features['Target']
    
    # Time-aware split (last 20% for testing)
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    print(f"Train set size: {len(X_train)}, Test set size: {len(X_test)}")

    # 4. Feature Scaling
    print("\nStep 3: Scaling features...")
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # 5. Run hyperparameter tuning for each model
    for model_type in ['RandomForest', 'XGBoost']:
        print(f"\n----- Processing Model: {model_type} -----")
        
        # Run tuning to get the best model and its parameters
        best_model, best_params = run_tuning(X_train_scaled, y_train, model_type)
        
        print(f"\nBest Hyperparameters for {model_type}:")
        print(best_params)
        
        # 6. Evaluate the final model on the test set
        y_pred = best_model.predict(X_test_scaled)
        
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mean_squared_error(y_test, y_pred))
        mae = mean_absolute_error(y_test, y_pred)
        
        print("\nFinal Evaluation Metrics on Test Set:")
        print(f"  RÂ² Score: {r2:.4f}")
        print(f"  MAE:      {mae:.4f}")
        print(f"  RMSE:     {rmse:.4f}\n")
        
        # 7. Visualize feature importance
        print("Displaying feature importance plot...")
        plot_feature_importance(best_model, features, model_type)

if __name__ == "__main__":
    # You can change the ticker symbol here
    main(ticker="AAPL") 